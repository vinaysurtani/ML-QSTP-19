{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-class Classification and Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "In this exercise, you will implement one-vs-all logistic regression and neural networks to recognize handwritten digits. Before starting the programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics. \n",
    "\n",
    "All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook. \n",
    "\n",
    "Before we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using [`numpy`](http://www.numpy.org/) for all arrays and matrix operations, [`matplotlib`](https://matplotlib.org/) for plotting, and [`scipy`](https://docs.scipy.org/doc/scipy/reference/) for scientific and numerical computation functions and tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import utils\n",
    "import utils2\n",
    "\n",
    "\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission and Grading\n",
    "\n",
    "\n",
    "After completing each part of the assignment, be sure to submit your solutions. The following is a breakdown of how each part of this exercise is scored.\n",
    "\n",
    "\n",
    "| Section | Part                                 | Submission function                   |  Points \n",
    "| :-      |:-                                    | :-                                    |  :-:    \n",
    "| 1       | [Regularized Logistic Regression](#section1)     | [`lrCostFunction`](#lrCostFunction)   | 30     \n",
    "| 2       | [One-vs-all classifier training](#section2)       | [`oneVsAll`](#oneVsAll)               | 20     \n",
    "| 3       | [One-vs-all classifier prediction](#section3)     | [`predictOneVsAll`](#predictOneVsAll) | 20     \n",
    "| 4       | [Neural Network Prediction Function](#section4)   | [`predict`](#predict)           | 30\n",
    "|         | Total Points                         |                                 | 100    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Multi-class Classification\n",
    "\n",
    "For this exercise, you will use logistic regression and neural networks to recognize handwritten digits (from 0 to 9). Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes)\n",
    "on mail envelopes to recognizing amounts written on bank checks. This exercise will show you how the methods you have learned can be used for this classification task.\n",
    "\n",
    "In the first part of the exercise, you will extend your previous implementation of logistic regression and apply it to one-vs-all classification.\n",
    "\n",
    "### 1.1 Dataset\n",
    "\n",
    "You are given a data set in `ex3data1.mat` that contains 5000 training examples of handwritten digits (This is a subset of the [MNIST](http://yann.lecun.com/exdb/mnist) handwritten digit dataset). The `.mat` format means that that the data has been saved in a native Octave/MATLAB matrix format, instead of a text (ASCII) format like a csv-file. We use the `.mat` format here because this is the dataset provided in the MATLAB version of this assignment. Fortunately, python provides mechanisms to load MATLAB native format using the `loadmat` function within the `scipy.io` module. This function returns a python dictionary with keys containing the variable names within the `.mat` file. \n",
    "\n",
    "There are 5000 training examples in `ex3data1.mat`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix `X`. This gives us a 5000 by 400 matrix `X` where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\: (x^{(1)})^T \\: - \\\\ -\\: (x^{(2)})^T \\:- \\\\ \\vdots \\\\ - \\: (x^{(m)})^T \\:-  \\end{bmatrix} $$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector `y` that contains labels for the training set. \n",
    "We start the exercise by first loading the dataset. Execute the cell below, you do not need to write any code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20x20 Input Images of Digits\n",
    "input_layer_size  = 400\n",
    "\n",
    "# 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "num_labels = 10\n",
    "\n",
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('Data1', 'ex3data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "m = y.size\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualizing the data\n",
    "\n",
    "You will begin by visualizing a subset of the training set. In the following cell, the code randomly selects selects 100 rows from `X` and passes those rows to the `displayData` function. This function maps each row to a 20 pixel by 20 pixel grayscale image and displays the images together. We have provided the `displayData` function in the file `utils.py`. You are encouraged to examine the code to see how it works. Run the following cell to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Vectorizing Logistic Regression\n",
    "\n",
    "You will be using multiple one-vs-all logistic regression models to build a multi-class classifier. Since there are 10 classes, you will need to train 10 separate logistic regression classifiers. To make this training efficient, it is important to ensure that your code is well vectorized. In this section, you will implement a vectorized version of logistic regression that does not employ any `for` loops. You can use your code in the previous exercise as a starting point for this exercise. \n",
    "\n",
    "To test your vectorized logistic regression, we will use custom data as defined in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test values for the parameters theta\n",
    "theta_t = np.array([-2, -1, 1, 2], dtype=float)\n",
    "\n",
    "# test values for the inputs\n",
    "X_t = np.concatenate([np.ones((5, 1)), np.arange(1, 16).reshape(5, 3, order='F')/10.0], axis=1)\n",
    "\n",
    "# test values for the labels\n",
    "y_t = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "# test value for the regularization parameter\n",
    "lambda_t = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "#### 1.3.1 Vectorizing the cost function \n",
    "\n",
    "We will begin by writing a vectorized version of the cost function. Recall that in (unregularized) logistic regression, the cost function is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left( h_\\theta\\left( x^{(i)} \\right) \\right) - \\left(1 - y^{(i)} \\right) \\log \\left(1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] $$\n",
    "\n",
    "To compute each element in the summation, we have to compute $h_\\theta(x^{(i)})$ for every example $i$, where $h_\\theta(x^{(i)}) = g(\\theta^T x^{(i)})$ and $g(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function. It turns out that we can compute this quickly for all our examples by using matrix multiplication. Let us define $X$ and $\\theta$ as\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T - \\\\ - \\left( x^{(2)} \\right)^T - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T - \\end{bmatrix} \\qquad \\text{and} \\qquad \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} $$\n",
    "\n",
    "Then, by computing the matrix product $X\\theta$, we have: \n",
    "\n",
    "$$ X\\theta = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T\\theta - \\\\ - \\left( x^{(2)} \\right)^T\\theta - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T\\theta - \\end{bmatrix} = \\begin{bmatrix} - \\theta^T x^{(1)}  - \\\\ - \\theta^T x^{(2)} - \\\\ \\vdots \\\\ - \\theta^T x^{(m)}  - \\end{bmatrix} $$\n",
    "\n",
    "In the last equality, we used the fact that $a^Tb = b^Ta$ if $a$ and $b$ are vectors. This allows us to compute the products $\\theta^T x^{(i)}$ for all our examples $i$ in one line of code.\n",
    "\n",
    "#### 1.3.2 Vectorizing the gradient\n",
    "\n",
    "Recall that the gradient of the (unregularized) logistic regression cost is a vector where the $j^{th}$ element is defined as\n",
    "\n",
    "$$ \\frac{\\partial J }{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\left( h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_j^{(i)} \\right) $$\n",
    "\n",
    "To vectorize this operation over the dataset, we start by writing out all the partial derivatives explicitly for all $\\theta_j$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_n}\n",
    "\\end{bmatrix} = &\n",
    "\\frac{1}{m} \\begin{bmatrix}\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_0^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_1^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_2^{(i)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_n^{(i)}\\right) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= & \\frac{1}{m} \\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x^{(i)}\\right) \\\\\n",
    "= & \\frac{1}{m} X^T \\left( h_\\theta(x) - y\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$  h_\\theta(x) - y = \n",
    "\\begin{bmatrix}\n",
    "h_\\theta\\left(x^{(1)}\\right) - y^{(1)} \\\\\n",
    "h_\\theta\\left(x^{(2)}\\right) - y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta\\left(x^{(m)}\\right) - y^{(m)} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Note that $x^{(i)}$ is a vector, while $h_\\theta\\left(x^{(i)}\\right) - y^{(i)}$  is a scalar (single number).\n",
    "To understand the last step of the derivation, let $\\beta_i = (h_\\theta\\left(x^{(m)}\\right) - y^{(m)})$ and\n",
    "observe that:\n",
    "\n",
    "$$ \\sum_i \\beta_ix^{(i)} = \\begin{bmatrix} \n",
    "| & | & & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix} = x^T \\beta\n",
    "$$\n",
    "\n",
    "where the values $\\beta_i = \\left( h_\\theta(x^{(i)} - y^{(i)} \\right)$.\n",
    "\n",
    "The expression above allows us to compute all the partial derivatives\n",
    "without any loops. If you are comfortable with linear algebra, we encourage you to work through the matrix multiplications above to convince yourself that the vectorized version does the same computations. \n",
    "\n",
    "Your job is to write the unregularized cost function `lrCostFunction` which returns both the cost function $J(\\theta)$ and its gradient $\\frac{\\partial J}{\\partial \\theta}$. Your implementation should use the strategy we presented above to calculate $\\theta^T x^{(i)}$. You should also use a vectorized approach for the rest of the cost function. A fully vectorized version of `lrCostFunction` should not contain any loops.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Debugging Tip:** Vectorizing code can sometimes be tricky. One common strategy for debugging is to print out the sizes of the matrices you are working with using the `shape` property of `numpy` arrays. For example, given a data matrix $X$ of size $100 \\times 20$ (100 examples, 20 features) and $\\theta$, a vector with size $20$, you can observe that `np.dot(X, theta)` is a valid multiplication operation, while `np.dot(theta, X)` is not. Furthermore, if you have a non-vectorized version of your code, you can compare the output of your vectorized code and non-vectorized code to make sure that they produce the same outputs.\n",
    "</div>\n",
    "<a id=\"lrCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunction(theta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the cost of using theta as the parameter for regularized\n",
    "    logistic regression and the gradient of the cost w.r.t. to the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Logistic regression parameters. A vector with shape (n, ). n is \n",
    "        the number of features including any intercept.  \n",
    "    \n",
    "    X : array_like\n",
    "        The data set with shape (m x n). m is the number of examples, and\n",
    "        n is the number of features (including intercept).\n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape (m, ).\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
    "    Compute the partial derivatives and set grad to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta\n",
    "    \n",
    "    Hint 1\n",
    "    ------\n",
    "    The computation of the cost function and gradients can be efficiently\n",
    "    vectorized. For example, consider the computation\n",
    "    \n",
    "        sigmoid(X * theta)\n",
    "    \n",
    "    Each row of the resulting matrix will contain the value of the prediction\n",
    "    for that example. You can make use of this to vectorize the cost function\n",
    "    and gradient computations. \n",
    "    \n",
    "    Hint 2\n",
    "    ------\n",
    "    When computing the gradient of the regularized cost function, there are\n",
    "    many possible vectorized solutions, but one solution looks like:\n",
    "    \n",
    "        grad = (unregularized gradient for logistic regression)\n",
    "        temp = theta \n",
    "        temp[0] = 0   # because we don't add anything for j = 0\n",
    "        grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "    \n",
    "    Hint 3\n",
    "    ------\n",
    "    We have provided the implementatation of the sigmoid function within \n",
    "    the file `utils.py`. At the start of the notebook, we imported this file\n",
    "    as a module. Thus to access the sigmoid function within that file, you can\n",
    "    do the following: `utils.sigmoid(z)`.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Initialize some useful values\n",
    "    m = y.size\n",
    "    \n",
    "    # convert labels to ints if their type is bool\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g=utils.sigmoid(np.dot(X,theta))\n",
    "    t1=np.dot(y,np.log(g))\n",
    "    t2=np.dot((1-y),np.log(1-g))\n",
    "    tc=theta\n",
    "    tc[0]=0\n",
    "    t3=(lambda_t/(2*m))*np.sum(tc**2)\n",
    "    J=(1/m)*np.sum(-t1-t2)+t3\n",
    "    grad=(1/m)*(np.dot((g-y),X))+(lambda_t/m)*tc\n",
    "    grad[0]=(1/m)*(np.sum(np.dot((g-y),X[:,0])))\n",
    "    # =============================================================\n",
    "    return J, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Vectorizing regularized logistic regression\n",
    "\n",
    "After you have implemented vectorization for logistic regression, you will now\n",
    "add regularization to the cost function. Recall that for regularized logistic\n",
    "regression, the cost function is defined as\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left(h_\\theta\\left(x^{(i)} \\right)\\right) - \\left( 1 - y^{(i)} \\right) \\log\\left(1 - h_\\theta \\left(x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not be regularizing $\\theta_0$ which is used for the bias term.\n",
    "Correspondingly, the partial derivative of regularized logistic regression cost for $\\theta_j$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}  & \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\text{for } j  \\ge 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now modify your code in lrCostFunction in the [**previous cell**](#lrCostFunction) to account for regularization. Once again, you should not put any loops into your code.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**python/numpy Tip:** When implementing the vectorization for regularized logistic regression, you might often want to only sum and update certain elements of $\\theta$. In `numpy`, you can index into the matrices to access and update only certain elements. For example, A[:, 3:5]\n",
    "= B[:, 1:3] will replaces the columns with index 3 to 5 of A with the columns with index 1 to 3 from B. To select columns (or rows) until the end of the matrix, you can leave the right hand side of the colon blank. For example, A[:, 2:] will only return elements from the $3^{rd}$ to last columns of $A$. If you leave the left hand size of the colon blank, you will select elements from the beginning of the matrix. For example, A[:, :2] selects the first two columns, and is equivalent to A[:, 0:2]. In addition, you can use negative indices to index arrays from the end. Thus, A[:, :-1] selects all columns of A except the last column, and A[:, -5:] selects the $5^{th}$ column from the end to the last column. Thus, you could use this together with the sum and power ($^{**}$) operations to compute the sum of only the elements you are interested in (e.g., `np.sum(z[1:]**2)`). In the starter code, `lrCostFunction`, we have also provided hints on yet another possible method computing the regularized gradient.\n",
    "</div>\n",
    "\n",
    "Once you finished your implementation, you can call the function `lrCostFunction` to test your solution using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost         : 2.534819\n",
      "Expected cost: 2.534819\n",
      "-----------------------\n",
      "Gradients:\n",
      " [0.146561, -0.548558, 0.724722, 1.398003]\n",
      "Expected gradients:\n",
      " [0.146561, -0.548558, 0.724722, 1.398003]\n"
     ]
    }
   ],
   "source": [
    "J, grad = lrCostFunction(theta_t, X_t, y_t, lambda_t)\n",
    "\n",
    "print('Cost         : {:.6f}'.format(J))\n",
    "print('Expected cost: 2.534819')\n",
    "print('-----------------------')\n",
    "print('Gradients:')\n",
    "print(' [{:.6f}, {:.6f}, {:.6f}, {:.6f}]'.format(*grad))\n",
    "print('Expected gradients:')\n",
    "print(' [0.146561, -0.548558, 0.724722, 1.398003]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing a part of the exercise, you can submit your solutions for grading by first adding the function you modified to the submission object, and then sending your function to Coursera for grading. \n",
    "\n",
    "The submission script will prompt you for your login e-mail and submission token. You can obtain a submission token from the web page for the assignment. You are allowed to submit your solutions multiple times, and we will take only the highest score into consideration.\n",
    "\n",
    "*Execute the following cell to grade your solution to the first part of this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go\n"
     ]
    }
   ],
   "source": [
    "\n",
    "allow = 0.5\n",
    "if(J - 2.534819 > allow):\n",
    "    print(\"Your error is too much. Consider checking your code\")\n",
    "else:\n",
    "    print(\"You are good to go\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 1.4 One-vs-all Classification\n",
    "\n",
    "In this part of the exercise, you will implement one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the $K$ classes in our dataset. In the handwritten digits dataset, $K = 10$, but your code should work for any value of $K$. \n",
    "\n",
    "You should now complete the code for the function `oneVsAll` below, to train one classifier for each class. In particular, your code should return all the classifier parameters in a matrix $\\theta \\in \\mathbb{R}^{K \\times (N +1)}$, where each row of $\\theta$ corresponds to the learned logistic regression parameters for one class. You can do this with a “for”-loop from $0$ to $K-1$, training each classifier independently.\n",
    "\n",
    "Note that the `y` argument to this function is a vector of labels from 0 to 9. When training the classifier for class $k \\in \\{0, ..., K-1\\}$, you will want a K-dimensional vector of labels $y$, where $y_j \\in 0, 1$ indicates whether the $j^{th}$ training instance belongs to class $k$ $(y_j = 1)$, or if it belongs to a different\n",
    "class $(y_j = 0)$. You may find logical arrays helpful for this task. \n",
    "\n",
    "Furthermore, you will be using scipy's `optimize.minimize` for this exercise. \n",
    "<a id=\"oneVsAll\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lambda_):\n",
    "    \"\"\"\n",
    "    Trains num_labels logistic regression classifiers and returns\n",
    "    each of these classifiers in a matrix all_theta, where the i-th\n",
    "    row of all_theta corresponds to the classifier for label i.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n). m is the number of \n",
    "        data points, and n is the number of features. Note that we \n",
    "        do not assume that the intercept term (or bias) is in X, however\n",
    "        we provide the code below to add the bias term to X. \n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector of shape (m, ).\n",
    "    \n",
    "    num_labels : int\n",
    "        Number of possible labels.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The logistic regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_theta : array_like\n",
    "        The trained parameters for logistic regression for each class.\n",
    "        This is a matrix of shape (K x n+1) where K is number of classes\n",
    "        (ie. `numlabels`) and n is number of features without the bias.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the following code to train `num_labels`\n",
    "    logistic regression classifiers with regularization parameter `lambda_`. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You can use y == c to obtain a vector of 1's and 0's that tell you\n",
    "    whether the ground truth is true/false for this class.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    For this assignment, we recommend using `scipy.optimize.minimize(method='CG')`\n",
    "    to optimize the cost function. It is okay to use a for-loop \n",
    "    (`for c in range(num_labels):`) to loop over the different classes.\n",
    "    \n",
    "    Example Code\n",
    "    ------------\n",
    "    \n",
    "        # Set Initial theta\n",
    "        initial_theta = np.zeros(n + 1)\n",
    "      \n",
    "        # Set options for minimize\n",
    "        options = {'maxiter': 50}\n",
    "    \n",
    "        # Run minimize to obtain the optimal theta. This function will \n",
    "        # return a class object where theta is in `res.x` and cost in `res.fun`\n",
    "        res = optimize.minimize(lrCostFunction, \n",
    "                                initial_theta, \n",
    "                                (X, (y == c), lambda_), \n",
    "                                jac=True, \n",
    "                                method='TNC',\n",
    "                                options=options) \n",
    "    \"\"\"\n",
    "    # Some useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    all_theta = np.zeros((num_labels, n + 1))\n",
    "\n",
    "    # Add ones to the X data matrix\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    #lrCostFunction(theta_t, X_t, y_t, lambda_t)\n",
    "    for i in range(num_labels):\n",
    "        initial_theta=np.zeros((X.shape[1],1))\n",
    "        y_true=(y==i)\n",
    "        res = optimize.minimize(lrCostFunction, \n",
    "                                initial_theta, \n",
    "                                (X, y_true, lambda_), \n",
    "                                jac=True, \n",
    "                                method='BFGS')\n",
    "        all_theta[i]=(res.x)\n",
    "    # ============================================================\n",
    "    return all_theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code for `oneVsAll`, the following cell will use your implementation to train a multi-class classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "all_theta = oneVsAll(X, y, num_labels, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.72503673e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.34739932e-04,  8.33288614e-06,  0.00000000e+00],\n",
       "       [-1.89901444e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         4.42534829e-04, -3.96419688e-09,  0.00000000e+00],\n",
       "       [-3.08500548e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         2.07072680e-03, -2.37006924e-04,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-1.78213428e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -2.24411367e-04,  2.79686750e-05,  0.00000000e+00],\n",
       "       [-7.45662196e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -4.68174269e-05,  2.91850256e-06,  0.00000000e+00],\n",
       "       [-4.21618661e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -2.92610421e-04,  2.13310019e-05,  0.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "#### 1.4.1 One-vs-all Prediction\n",
    "\n",
    "After training your one-vs-all classifier, you can now use it to predict the digit contained in a given image. For each input, you should compute the “probability” that it belongs to each class using the trained logistic regression classifiers. Your one-vs-all prediction function will pick the class for which the corresponding logistic regression classifier outputs the highest probability and return the class label (0, 1, ..., K-1) as the prediction for the input example. You should now complete the code in the function `predictOneVsAll` to use the one-vs-all classifier for making predictions. \n",
    "<a id=\"predictOneVsAll\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "    \"\"\"\n",
    "    Return a vector of predictions for each example in the matrix X. \n",
    "    Note that X contains the examples in rows. all_theta is a matrix where\n",
    "    the i-th row is a trained logistic regression theta vector for the \n",
    "    i-th class. You should set p to a vector of values from 0..K-1 \n",
    "    (e.g., p = [0, 2, 0, 1] predicts classes 0, 2, 0, 1 for 4 examples) .\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_theta : array_like\n",
    "        The trained parameters for logistic regression for each class.\n",
    "        This is a matrix of shape (K x n+1) where K is number of classes\n",
    "        and n is number of features without the bias.\n",
    "    \n",
    "    X : array_like\n",
    "        Data points to predict their labels. This is a matrix of shape \n",
    "        (m x n) where m is number of data points to predict, and n is number \n",
    "        of features without the bias term. Note we add the bias term for X in \n",
    "        this function. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        The predictions for each data point in X. This is a vector of shape (m, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned logistic\n",
    "    regression parameters (one-vs-all). You should set p to a vector of predictions\n",
    "    (from 0 to num_labels-1).\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    This code can be done all vectorized using the numpy argmax function.\n",
    "    In particular, the argmax function returns the index of the max element,\n",
    "    for more information see '?np.argmax' or search online. If your examples\n",
    "    are in rows, then, you can use np.argmax(A, axis=1) to obtain the index \n",
    "    of the max for each row.\n",
    "    \"\"\"\n",
    "    m = X.shape[0];\n",
    "    num_labels = all_theta.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # Add ones to the X data matrix\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    h=utils.sigmoid(np.dot(X,np.transpose(all_theta)))\n",
    "    p=np.argmax(h,axis=1)\n",
    "    # ============================================================\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, call your `predictOneVsAll` function using the learned value of $\\theta$. You should see that the training set accuracy is about 95.1% (i.e., it classifies 95.1% of the examples in the training set correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 93.30%\n"
     ]
    }
   ],
   "source": [
    "pred = predictOneVsAll(all_theta, X)\n",
    "print('Training Set Accuracy: {:.2f}%'.format(np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(pred == y) * 100\n",
    "if(95.1 - acc > 2):\n",
    "    print(\"Consider checking your code\")\n",
    "    \n",
    "else:\n",
    "    print(\"You are good to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Neural Networks\n",
    "\n",
    "In the previous part of this exercise, you implemented multi-class logistic regression to recognize handwritten digits. However, logistic regression cannot form more complex hypotheses as it is only a linear classifier (You could add more features - such as polynomial features - to logistic regression, but that can be very expensive to train).\n",
    "\n",
    "In this part of the exercise, you will implement a neural network to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses. For this week, you will be using parameters from a neural network that we have already trained. Your goal is to implement the feedforward propagation algorithm to use our weights for prediction. In next week’s exercise, you will write the backpropagation algorithm for learning the neural network parameters. \n",
    "\n",
    "We start by first reloading and visualizing the dataset which contains the MNIST handwritten digits (this is the same as we did in the first part of this exercise, we reload it here to ensure the variables have not been modified). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('Data1', 'ex3data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# get number of examples in dataset\n",
    "m = y.size\n",
    "\n",
    "# randomly permute examples, to be used for visualizing one \n",
    "# picture at a time\n",
    "indices = np.random.permutation(m)\n",
    "\n",
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Model representation \n",
    "\n",
    "Our neural network is shown in the following figure.\n",
    "\n",
    "![Neural network](Figures/neural_network.png)\n",
    "\n",
    "It has 3 layers: an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size 20×20, this gives us 400 input layer units (excluding the extra bias unit which always outputs +1). As before, the training data will be loaded into the variables X and y. \n",
    "\n",
    "You have been provided with a set of network parameters ($\\Theta^{(1)}$, $\\Theta^{(2)}$) already trained by us. These are stored in `ex3weights.mat`. The following cell loads those parameters into  `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the .mat file, which returns a dictionary \n",
    "weights = loadmat(os.path.join('Data1', 'ex3weights.mat'))\n",
    "\n",
    "# get the model weights from the dictionary\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.2 Feedforward Propagation and Prediction\n",
    "\n",
    "Now you will implement feedforward propagation for the neural network. You will need to complete the code in the function `predict` to return the neural network’s prediction. You should implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example $i$ and returns the associated predictions. Similar to the one-vs-all classification strategy, the prediction from the neural network will be the label that has the largest output $\\left( h_\\theta(x) \\right)_k$.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Implementation Note:** The matrix $X$ contains the examples in rows. When you complete the code in the function `predict`, you will need to add the column of 1’s to the matrix. The matrices `Theta1` and `Theta2` contain the parameters for each unit in rows. Specifically, the first row of `Theta1` corresponds to the first hidden unit in the second layer. In `numpy`, when you compute $z^{(2)} = \\theta^{(1)}a^{(1)}$, be sure that you index (and if necessary, transpose) $X$ correctly so that you get $a^{(l)}$ as a 1-D vector.\n",
    "</div>\n",
    "<a id=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"\n",
    "    Predict the label of an input given a trained neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Theta1 : array_like\n",
    "        Weights for the first layer in the neural network.\n",
    "        It has shape (2nd hidden layer size x input size)\n",
    "    \n",
    "    Theta2: array_like\n",
    "        Weights for the second layer in the neural network. \n",
    "        It has shape (output layer size x 2nd hidden layer size)\n",
    "    \n",
    "    X : array_like\n",
    "        The image inputs having shape (number of examples x image dimensions).\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    p : array_like\n",
    "        Predictions vector containing the predicted label for each example.\n",
    "        It has a length equal to the number of examples.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned neural\n",
    "    network. You should set p to a vector containing labels \n",
    "    between 0 to (num_labels-1).\n",
    "     \n",
    "    Hint\n",
    "    ----\n",
    "    This code can be done all vectorized using the numpy argmax function.\n",
    "    In particular, the argmax function returns the index of the  max element,\n",
    "    for more information see '?np.argmax' or search online. If your examples\n",
    "    are in rows, then, you can use np.argmax(A, axis=1) to obtain the index\n",
    "    of the max for each row.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Remember, we have supplied the `sigmoid` function in the `utils.py` file. \n",
    "    You can use this function by calling `utils.sigmoid(z)`, where you can \n",
    "    replace `z` by the required input variable to sigmoid.\n",
    "    \"\"\"\n",
    "    # Make sure the input has two dimensions\n",
    "    if X.ndim == 1:\n",
    "        X = X[None]  # promote to 2-dimensions\n",
    "    \n",
    "    # useful variables\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    p = np.zeros(X.shape[0])\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    z2=np.dot(X,np.transpose(Theta1[:,1:401]))\n",
    "    \n",
    "    a2=np.zeros((5000,26))\n",
    "    a2[:,1:26]=utils.sigmoid(z2)\n",
    "    a2[:,0]=1\n",
    "    \n",
    "    z3=np.dot(a2,np.transpose(Theta2))\n",
    "    h=utils.sigmoid(z3)\n",
    "    p=np.argmax(h,axis=1)\n",
    "    # =============================================================\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, call your predict function using the loaded set of parameters for `Theta1` and `Theta2`. You should see that the accuracy is about 97.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 97.3%\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: {:.1f}%'.format(np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will display images from the training set one at a time, while at the same time printing out the predicted label for the displayed image. \n",
    "\n",
    "Run the following cell to display a single image the the neural network's prediction. You can run the cell multiple time to see predictions for different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Prediction: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if indices.size > 0:\n",
    "    i, indices = indices[0], indices[1:]\n",
    "    utils.displayData(X[i, :], figsize=(4, 4))\n",
    "    pred = predict(Theta1, Theta2, X[i, :])\n",
    "    print('Neural Network Prediction: {}'.format(*pred))\n",
    "else:\n",
    "    print('No more images to display!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider checking your code\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(pred == y) * 100\n",
    "if(97.5 - acc > 2):\n",
    "    print(\"Consider checking your code\")\n",
    "else:\n",
    "    print(\"You are good to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission and Grading\n",
    "\n",
    "\n",
    "After completing each part of the assignment, be sure to submit your solutions. The following is a breakdown of how each part of this exercise is scored.\n",
    "\n",
    "\n",
    "| Section | Part                                             | Submission function | Points \n",
    "| :-      |:-                                                | :-                  | :-:    \n",
    "| 1       | [Feedforward and Cost Function](#section1)                    | [`nnCostFunction`](#nnCostFunction)   | 30     \n",
    "| 2       | [Regularized Cost Function](#section2)                        | [`nnCostFunction`](#nnCostFunction)   | 15     \n",
    "| 3       | [Sigmoid Gradient](#section3)                                 | [`sigmoidGradient`](#sigmoidGradient) | 5      \n",
    "| 4       | [Neural Net Gradient Function (Backpropagation)](#section4)   | [`nnCostFunction`](#nnCostFunction)   | 40     \n",
    "| 5       | [Regularized Gradient](#section5)                             | [`nnCostFunction`](#nnCostFunction)   |10     \n",
    "|         | Total Points                                     |    | 100    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In the previous part, you implemented feedforward propagation for neural networks and used it to predict handwritten digits with the weights we provided. In this part, you will implement the backpropagation algorithm to learn the parameters for the neural network.\n",
    "\n",
    "We have already loaded the dataset. We just need to load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat(os.path.join('Data2', 'ex4weights.mat'))\n",
    "\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "### 1.1 Feedforward and cost function\n",
    "\n",
    "Now you will implement the cost function and gradient for the neural network. First, complete the code for the function `nnCostFunction` in the next cell to return the cost.\n",
    "\n",
    "Recall that the cost function for the neural network (without regularization) is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "where $h_\\theta \\left( x^{(i)} \\right)$ is computed as shown in the neural network figure above, and K = 10 is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output\n",
    "value) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable y) were 0, 1, ..., 9, for the purpose of training a neural network, we need to encode the labels as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$ y = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (that you should use with the cost function) should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0.\n",
    "\n",
    "You should implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example $i$ and sum the cost over all examples. **Your code should also work for a dataset of any size, with any number of labels** (you can assume that there are always at least $K \\ge 3$ labels).\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Implementation Note:** The matrix $X$ contains the examples in rows (i.e., X[i,:] is the i-th training example $x^{(i)}$, expressed as a $n \\times 1$ vector.) When you complete the code in `nnCostFunction`, you will need to add the column of 1’s to the X matrix. The parameters for each unit in the neural network is represented in Theta1 and Theta2 as one row. Specifically, the first row of Theta1 corresponds to the first hidden unit in the second layer. You can use a for-loop over the examples to compute the cost.\n",
    "</div>\n",
    "<a id=\"nnCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "Use the following links to go back to the different parts of this exercise that require to modify the function `nnCostFunction`.<br>\n",
    "\n",
    "Back to:\n",
    "- [Feedforward and cost function](#section1)\n",
    "- [Regularized cost](#section2)\n",
    "- [Neural Network Gradient (Backpropagation)](#section4)\n",
    "- [Regularized Gradient](#section5)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the code by working through the following parts.\n",
    "    \n",
    "    - Part 1: Feedforward the neural network and return the cost in the \n",
    "              variable J. After implementing Part 1, you can verify that your\n",
    "              cost function computation is correct by verifying the cost\n",
    "              computed in the following cell.\n",
    "    \n",
    "    - Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "              Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "              the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "              Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "              that your implementation is correct by running checkNNGradients provided\n",
    "              in the utils.py module.\n",
    "     \n",
    "      Part 3: Implement regularization with the cost function and gradients.\n",
    "              \n",
    "    \n",
    "    \n",
    "    Note \n",
    "    ----\n",
    "    We have provided an implementation for the sigmoid function in the file \n",
    "    `utils.py` accompanying this assignment.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "    theta1c = Theta1.copy()\n",
    "    theta1c[:,0] = 0\n",
    "    theta1 = theta1c.flatten()\n",
    "    \n",
    "    \n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "    theta2c = Theta2.copy()\n",
    "    theta2c[:,0] = 0\n",
    "    theta2 = theta2c.flatten()\n",
    "    \n",
    "    \n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "    \n",
    "    lin_cost = lambda_*(np.dot(theta1,theta1)+np.dot(theta2,theta2))/(2*m)\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    l_m = range(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    z2 = (np.dot(a1,Theta1.T))\n",
    "    a2 = utils.sigmoid(z2)\n",
    "    a2 = np.concatenate([np.ones((m, 1)), a2], axis=1)\n",
    "    z_x = (np.dot(a2,Theta2.T))\n",
    "    h_x = utils.sigmoid(z_x)\n",
    "    print(h_x)\n",
    "    for i in l_m:\n",
    "      Y = np.zeros(num_labels)\n",
    "      Y[y[i]]=1\n",
    "      J += np.dot(Y,np.log(h_x[i]))+np.dot(1-Y,np.log(1-h_x[i]))\n",
    "    \n",
    "    # ================================================================\n",
    "    # Unroll gradients\n",
    "    # grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return (lin_cost-J/m), grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, call your `nnCostFunction` using the loaded set of parameters for `Theta1` and `Theta2`. You should see that the cost is about 0.287629."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.95734012e-01 1.12661530e-04 1.74127856e-03 ... 5.51517524e-03\n",
      "  4.01468105e-04 6.48072305e-03]\n",
      " [9.95696931e-01 4.79026796e-04 2.41495958e-03 ... 1.15788527e-02\n",
      "  2.39107046e-03 1.97025086e-03]\n",
      " [9.28008397e-01 8.85702310e-05 3.24266731e-03 ... 3.86839058e-04\n",
      "  6.22892325e-02 5.49803551e-03]\n",
      " ...\n",
      " [2.42384687e-05 5.17641791e-02 3.81715020e-03 ... 1.44301919e-04\n",
      "  2.15667361e-03 6.49826950e-01]\n",
      " [2.06173648e-04 8.30631310e-04 6.22003774e-04 ... 1.20516046e-02\n",
      "  1.19366192e-02 9.71410499e-01]\n",
      " [8.18576980e-02 4.81465717e-05 4.58821829e-04 ... 3.69700393e-02\n",
      "  5.73434571e-03 6.96288990e-01]]\n",
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "The cost should be about                   : 0.287629.\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f ' % J)\n",
    "print('The cost should be about                   : 0.287629.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to move on\n"
     ]
    }
   ],
   "source": [
    "if (J - 0.287629 > 0.3):\n",
    "    print(\"You should probably check your code\")\n",
    "    \n",
    "else:\n",
    "    print(\"You are good to move on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 1.2 Regularized cost function\n",
    "\n",
    "The cost function for neural networks with regularization is given by:\n",
    "\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n",
    "\n",
    "You can assume that the neural network will only have 3 layers - an input layer, a hidden layer and an output layer. However, your code should work for any number of input units, hidden units and outputs units. While we\n",
    "have explicitly listed the indices above for $\\Theta^{(1)}$ and $\\Theta^{(2)}$ for clarity, do note that your code should in general work with $\\Theta^{(1)}$ and $\\Theta^{(2)}$ of any size. Note that you should not be regularizing the terms that correspond to the bias. For the matrices `Theta1` and `Theta2`, this corresponds to the first column of each matrix. You should now add regularization to your cost function. Notice that you can first compute the unregularized cost function $J$ using your existing `nnCostFunction` and then later add the cost for the regularization terms.\n",
    "\n",
    "[Click here to go back to `nnCostFunction` for editing.](#nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, the next cell will call your `nnCostFunction` using the loaded set of parameters for `Theta1` and `Theta2`, and $\\lambda = 1$. You should see that the cost is about 0.383770."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.95734012e-01 1.12661530e-04 1.74127856e-03 ... 5.51517524e-03\n",
      "  4.01468105e-04 6.48072305e-03]\n",
      " [9.95696931e-01 4.79026796e-04 2.41495958e-03 ... 1.15788527e-02\n",
      "  2.39107046e-03 1.97025086e-03]\n",
      " [9.28008397e-01 8.85702310e-05 3.24266731e-03 ... 3.86839058e-04\n",
      "  6.22892325e-02 5.49803551e-03]\n",
      " ...\n",
      " [2.42384687e-05 5.17641791e-02 3.81715020e-03 ... 1.44301919e-04\n",
      "  2.15667361e-03 6.49826950e-01]\n",
      " [2.06173648e-04 8.30631310e-04 6.22003774e-04 ... 1.20516046e-02\n",
      "  1.19366192e-02 9.71410499e-01]\n",
      " [8.18576980e-02 4.81465717e-05 4.58821829e-04 ... 3.69700393e-02\n",
      "  5.73434571e-03 6.96288990e-01]]\n",
      "Cost at parameters (loaded from ex4weights): 0.383770\n",
      "This value should be about                 : 0.383770.\n"
     ]
    }
   ],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f' % J)\n",
    "print('This value should be about                 : 0.383770.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to move on\n"
     ]
    }
   ],
   "source": [
    "if (J - 0.383770 > 0.3):\n",
    "    print(\"You should probably check your code\")\n",
    "    \n",
    "else:\n",
    "    print(\"You are good to move on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Backpropagation\n",
    "\n",
    "In this part of the exercise, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function. You will need to update the function `nnCostFunction` so that it returns an appropriate value for `grad`. Once you have computed the gradient, you will be able to train the neural network by minimizing the cost function $J(\\theta)$ using an advanced optimizer such as `scipy`'s `optimize.minimize`.\n",
    "You will first implement the backpropagation algorithm to compute the gradients for the parameters for the (unregularized) neural network. After you have verified that your gradient computation for the unregularized case is correct, you will implement the gradient for the regularized neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "### 2.1 Sigmoid Gradient\n",
    "\n",
    "To help you get started with this part of the exercise, you will first implement\n",
    "the sigmoid gradient function. The gradient for the sigmoid function can be\n",
    "computed as\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Now complete the implementation of `sigmoidGradient` in the next cell.\n",
    "<a id=\"sigmoidGradient\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    We have provided an implementation of the sigmoid function \n",
    "    in `utils.py` file accompanying this assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, the following cell call `sigmoidGradient` on a given vector `z`. Try testing a few values by calling `sigmoidGradient(z)`. For large values (both positive and negative) of z, the gradient should be close to 0. When $z = 0$, the gradient should be exactly 0.25. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid gradient function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "g = sigmoidGradient(z)\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have to check your code\n"
     ]
    }
   ],
   "source": [
    "ak = np.array([0.19661193, 0.23500371, 0.25, 0.23500371, 0.19661193])\n",
    "for a,m in enumerate(ak):\n",
    "    if(ak[a] - g[a] > 1e-02):\n",
    "        print(\"You have to check your code\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random Initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{l}$.\n",
    "</div>\n",
    "\n",
    "Your job is to complete the function `randInitializeWeights` to initialize the weights for $\\Theta$. Modify the function by filling in the following code:\n",
    "\n",
    "```python\n",
    "# Randomly initialize the weights to small values\n",
    "W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "```\n",
    "Note that we give the function an argument for $\\epsilon$ with default value `epsilon_init = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Initialize W randomly so that we break the symmetry while training\n",
    "    the neural network. Note that the first column of W corresponds \n",
    "    to the parameters for the bias unit.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You do not need to submit any code for this part of the exercise.*\n",
    "\n",
    "Execute the following cell to initialize the weights for the 2 layers in the neural network using the `randInitializeWeights` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.3 Backpropagation\n",
    "\n",
    "![](Figures/ex4-backpropagation.png)\n",
    "\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis $h_\\theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an “error term” $\\delta_j^{(l)}$ that measures how much that node was “responsible” for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$. In detail, here is the backpropagation algorithm (also depicted in the figure above). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for-loop `for t in range(m)` and place steps 1-4 below inside the for-loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)}, y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Set the input layer’s values $(a^{(1)})$ to the $t^{th }$training example $x^{(t)}$. Perform a feedforward pass, computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a `+1` term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. In `numpy`, if a 1 is a column matrix, adding one corresponds to `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), set \n",
    "$$\\delta_k^{(3)} = \\left(a_k^{(3)} - y_k \\right)$$\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$. You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "1. For the hidden layer $l = 2$, set \n",
    "$$ \\delta^{(2)} = \\left( \\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $*$ performs element wise multiplication in `numpy`.\n",
    "\n",
    "1. Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$. In `numpy`, removing $\\delta_0^{(2)}$ corresponds to `delta_2 = delta_2[1:]`.\n",
    "\n",
    "1. Obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Python/Numpy tip**: You should implement the backpropagation algorithm only after you have successfully completed the feedforward and cost functions. While implementing the backpropagation alogrithm, it is often useful to use the `shape` function to print out the shapes of the variables you are working with if you run into dimension mismatch errors.\n",
    "</div>\n",
    "\n",
    "[Click here to go back and update the function `nnCostFunction` with the backpropagation algorithm](#nnCostFunction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have implemented the backpropagation algorithm, we will proceed to run gradient checking on your implementation. The gradient check will allow you to increase your confidence that your code is\n",
    "computing the gradients correctly.\n",
    "\n",
    "### 2.4  Gradient checking \n",
    "\n",
    "In your neural network, you are minimizing the cost function $J(\\Theta)$. To perform gradient checking on your parameters, you can imagine “unrolling” the parameters $\\Theta^{(1)}$, $\\Theta^{(2)}$ into a long vector $\\theta$. By doing so, you can think of the cost function being $J(\\Theta)$ instead and use the following gradient checking procedure.\n",
    "\n",
    "Suppose you have a function $f_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; you’d like to check if $f_i$ is outputting correct derivative values.\n",
    "\n",
    "$$\n",
    "\\text{Let } \\theta^{(i+)} = \\theta + \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad \\theta^{(i-)} = \\theta - \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i^{th}$ element has been incremented by $\\epsilon$. Similarly, $\\theta^{(i−)}$ is the corresponding vector with the $i^{th}$ element decreased by $\\epsilon$. You can now numerically verify $f_i(\\theta)$’s correctness by checking, for each $i$, that:\n",
    "\n",
    "$$ f_i\\left( \\theta \\right) \\approx \\frac{J\\left( \\theta^{(i+)}\\right) - J\\left( \\theta^{(i-)} \\right)}{2\\epsilon} $$\n",
    "\n",
    "The degree to which these two values should approximate each other will depend on the details of $J$. But assuming $\\epsilon = 10^{-4}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more).\n",
    "\n",
    "We have implemented the function to compute the numerical gradient for you in `computeNumericalGradient` (within the file `utils.py`). While you are not required to modify the file, we highly encourage you to take a look at the code to understand how it works.\n",
    "\n",
    "In the next cell we will run the provided function `checkNNGradients` which will create a small neural network and dataset that will be used for checking your gradients. If your backpropagation implementation is correct,\n",
    "you should see a relative difference that is less than 1e-9.\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "**Practical Tip**: When performing gradient checking, it is much more efficient to use a small neural network with a relatively small number of input units and hidden units, thus having a relatively small number\n",
    "of parameters. Each dimension of $\\theta$ requires two evaluations of the cost function and this can be expensive. In the function `checkNNGradients`, our code creates a small random model and dataset which is used with `computeNumericalGradient` for gradient checking. Furthermore, after you are confident that your gradient computations are correct, you should turn off gradient checking before running your learning algorithm.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "**Practical Tip:** Gradient checking works for any function where you are computing the cost and the gradient. Concretely, you can use the same `computeNumericalGradient` function to check if your gradient implementations for the other exercises are correct too (e.g., logistic regression’s cost function).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51456751 0.51113259 0.4974603 ]\n",
      " [0.51458711 0.51113874 0.49744734]\n",
      " [0.51456257 0.51106634 0.49739362]\n",
      " [0.51451644 0.51098196 0.49734856]\n",
      " [0.51449122 0.51096332 0.49735366]]\n",
      "[[0.51456798 0.51113319 0.49746048]\n",
      " [0.51458758 0.51113934 0.49744751]\n",
      " [0.51456304 0.51106693 0.4973938 ]\n",
      " [0.51451692 0.51098256 0.49734874]\n",
      " [0.51449169 0.51096392 0.49735383]]\n",
      "[[0.51456704 0.51113199 0.49746013]\n",
      " [0.51458664 0.51113814 0.49744716]\n",
      " [0.51456209 0.51106574 0.49739345]\n",
      " [0.51451597 0.51098136 0.49734839]\n",
      " [0.51449075 0.51096272 0.49735348]]\n",
      "[[0.51456755 0.51113264 0.49746032]\n",
      " [0.51458715 0.51113879 0.49744735]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451641 0.51098191 0.49734855]\n",
      " [0.51449118 0.51096326 0.49735364]]\n",
      "[[0.51456747 0.51113254 0.49746029]\n",
      " [0.51458707 0.51113869 0.49744732]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451648 0.510982   0.49734858]\n",
      " [0.51449127 0.51096338 0.49735367]]\n",
      "[[0.5145675  0.51113257 0.4974603 ]\n",
      " [0.51458714 0.51113878 0.49744735]\n",
      " [0.51456261 0.51106639 0.49739364]\n",
      " [0.51451646 0.51098198 0.49734857]\n",
      " [0.5144912  0.51096329 0.49735365]]\n",
      "[[0.51456752 0.51113261 0.49746031]\n",
      " [0.51458708 0.5111387  0.49744733]\n",
      " [0.51456252 0.51106628 0.49739361]\n",
      " [0.51451643 0.51098193 0.49734856]\n",
      " [0.51449125 0.51096335 0.49735367]]\n",
      "[[0.51456746 0.51113253 0.49746029]\n",
      " [0.51458708 0.51113871 0.49744733]\n",
      " [0.51456259 0.51106636 0.49739363]\n",
      " [0.51451649 0.51098202 0.49734858]\n",
      " [0.51449125 0.51096336 0.49735367]]\n",
      "[[0.51456756 0.51113265 0.49746032]\n",
      " [0.51458713 0.51113877 0.49744735]\n",
      " [0.51456255 0.51106631 0.49739362]\n",
      " [0.5145164  0.5109819  0.49734855]\n",
      " [0.51449119 0.51096328 0.49735364]]\n",
      "[[0.5145671  0.51113197 0.49746005]\n",
      " [0.5145867  0.51113812 0.49744708]\n",
      " [0.51456216 0.51106572 0.49739337]\n",
      " [0.51451603 0.51098134 0.49734831]\n",
      " [0.51449081 0.5109627  0.4973534 ]]\n",
      "[[0.51456792 0.51113321 0.49746056]\n",
      " [0.51458752 0.51113936 0.49744759]\n",
      " [0.51456298 0.51106695 0.49739388]\n",
      " [0.51451685 0.51098258 0.49734882]\n",
      " [0.51449163 0.51096394 0.49735391]]\n",
      "[[0.51456747 0.51113254 0.49746028]\n",
      " [0.51458707 0.51113868 0.49744731]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451648 0.510982   0.49734858]\n",
      " [0.51449126 0.51096338 0.49735368]]\n",
      "[[0.51456754 0.51113264 0.49746033]\n",
      " [0.51458715 0.5111388  0.49744736]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451641 0.51098191 0.49734854]\n",
      " [0.51449118 0.51096326 0.49735363]]\n",
      "[[0.51456752 0.51113261 0.49746031]\n",
      " [0.51458708 0.5111387  0.49744732]\n",
      " [0.51456253 0.51106627 0.4973936 ]\n",
      " [0.51451643 0.51098193 0.49734855]\n",
      " [0.51449124 0.51096335 0.49735367]]\n",
      "[[0.5145675  0.51113257 0.4974603 ]\n",
      " [0.51458714 0.51113878 0.49744735]\n",
      " [0.51456261 0.5110664  0.49739365]\n",
      " [0.51451646 0.51098198 0.49734857]\n",
      " [0.5144912  0.51096329 0.49735364]]\n",
      "[[0.51456755 0.51113265 0.49746033]\n",
      " [0.51458713 0.51113877 0.49744735]\n",
      " [0.51456255 0.51106631 0.49739361]\n",
      " [0.5145164  0.5109819  0.49734854]\n",
      " [0.51449119 0.51096328 0.49735364]]\n",
      "[[0.51456747 0.51113253 0.49746028]\n",
      " [0.51458709 0.51113871 0.49744732]\n",
      " [0.51456258 0.51106636 0.49739363]\n",
      " [0.51451649 0.51098202 0.49734859]\n",
      " [0.51449125 0.51096336 0.49735367]]\n",
      "[[0.51456785 0.51113321 0.49746064]\n",
      " [0.51458745 0.51113936 0.49744767]\n",
      " [0.51456291 0.51106696 0.49739396]\n",
      " [0.51451678 0.51098258 0.4973489 ]\n",
      " [0.51449156 0.51096394 0.49735399]]\n",
      "[[0.51456717 0.51113196 0.49745997]\n",
      " [0.51458677 0.51113812 0.497447  ]\n",
      " [0.51456223 0.51106571 0.49739329]\n",
      " [0.5145161  0.51098133 0.49734823]\n",
      " [0.51449088 0.51096269 0.49735332]]\n",
      "[[0.51456754 0.51113264 0.49746033]\n",
      " [0.51458714 0.5111388  0.49744737]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451642 0.51098191 0.49734854]\n",
      " [0.51449119 0.51096326 0.49735362]]\n",
      "[[0.51456748 0.51113254 0.49746028]\n",
      " [0.51458708 0.51113868 0.49744731]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451647 0.51098201 0.49734859]\n",
      " [0.51449125 0.51096338 0.49735369]]\n",
      "[[0.5145675  0.51113257 0.49746029]\n",
      " [0.51458713 0.51113878 0.49744736]\n",
      " [0.5145626  0.5110664  0.49739366]\n",
      " [0.51451646 0.51098198 0.49734858]\n",
      " [0.5144912  0.51096328 0.49735364]]\n",
      "[[0.51456752 0.51113261 0.49746031]\n",
      " [0.51458709 0.5111387  0.49744732]\n",
      " [0.51456253 0.51106627 0.49739359]\n",
      " [0.51451643 0.51098193 0.49734855]\n",
      " [0.51449124 0.51096335 0.49735367]]\n",
      "[[0.51456748 0.51113253 0.49746027]\n",
      " [0.51458709 0.51113871 0.49744732]\n",
      " [0.51456258 0.51106636 0.49739364]\n",
      " [0.51451648 0.51098202 0.4973486 ]\n",
      " [0.51449124 0.51096336 0.49735368]]\n",
      "[[0.51456754 0.51113265 0.49746034]\n",
      " [0.51458713 0.51113877 0.49744736]\n",
      " [0.51456255 0.51106631 0.49739361]\n",
      " [0.51451641 0.5109819  0.49734853]\n",
      " [0.5144912  0.51096328 0.49735363]]\n",
      "[[0.51456725 0.51113197 0.4974599 ]\n",
      " [0.51458685 0.51113812 0.49744693]\n",
      " [0.5145623  0.51106572 0.49739322]\n",
      " [0.51451618 0.51098134 0.49734816]\n",
      " [0.51449096 0.5109627  0.49735325]]\n",
      "[[0.51456777 0.51113321 0.49746071]\n",
      " [0.51458737 0.51113936 0.49744774]\n",
      " [0.51456283 0.51106695 0.49739403]\n",
      " [0.51451671 0.51098258 0.49734897]\n",
      " [0.51449148 0.51096394 0.49735406]]\n",
      "[[0.51456749 0.51113254 0.49746027]\n",
      " [0.51458709 0.51113868 0.4974473 ]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451646 0.51098201 0.49734859]\n",
      " [0.51449125 0.51096338 0.4973537 ]]\n",
      "[[0.51456753 0.51113264 0.49746034]\n",
      " [0.51458713 0.5111388  0.49744737]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451642 0.51098191 0.49734853]\n",
      " [0.5144912  0.51096326 0.49735362]]\n",
      "[[0.51456752 0.51113261 0.49746032]\n",
      " [0.51458709 0.5111387  0.49744731]\n",
      " [0.51456254 0.51106627 0.49739358]\n",
      " [0.51451643 0.51098193 0.49734855]\n",
      " [0.51449124 0.51096335 0.49735368]]\n",
      "[[0.5145675  0.51113257 0.49746029]\n",
      " [0.51458713 0.51113878 0.49744736]\n",
      " [0.51456259 0.5110664  0.49739366]\n",
      " [0.51451646 0.51098198 0.49734858]\n",
      " [0.51449121 0.51096329 0.49735363]]\n",
      "[[0.51456754 0.51113265 0.49746034]\n",
      " [0.51458712 0.51113877 0.49744736]\n",
      " [0.51456256 0.51106631 0.49739361]\n",
      " [0.51451642 0.5109819  0.49734852]\n",
      " [0.5144912  0.51096328 0.49735363]]\n",
      "[[0.51456748 0.51113253 0.49746026]\n",
      " [0.5145871  0.51113871 0.49744732]\n",
      " [0.51456258 0.51106636 0.49739364]\n",
      " [0.51451647 0.51098202 0.4973486 ]\n",
      " [0.51449124 0.51096336 0.49735368]]\n",
      "[[0.51456769 0.51113319 0.49746077]\n",
      " [0.51458729 0.51113934 0.49744781]\n",
      " [0.51456275 0.51106693 0.49739409]\n",
      " [0.51451662 0.51098256 0.49734903]\n",
      " [0.5144914  0.51096392 0.49735412]]\n",
      "[[0.51456733 0.51113199 0.49745984]\n",
      " [0.51458693 0.51113814 0.49744687]\n",
      " [0.51456239 0.51106574 0.49739316]\n",
      " [0.51451627 0.51098136 0.49734809]\n",
      " [0.51449104 0.51096272 0.49735319]]\n",
      "[[0.51456752 0.51113264 0.49746034]\n",
      " [0.51458713 0.51113879 0.49744738]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451643 0.51098191 0.49734853]\n",
      " [0.5144912  0.51096326 0.49735361]]\n",
      "[[0.51456749 0.51113254 0.49746026]\n",
      " [0.51458709 0.51113869 0.49744729]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451646 0.510982   0.4973486 ]\n",
      " [0.51449124 0.51096338 0.4973537 ]]\n",
      "[[0.5145675  0.51113257 0.49746029]\n",
      " [0.51458712 0.51113878 0.49744737]\n",
      " [0.51456258 0.51106639 0.49739367]\n",
      " [0.51451645 0.51098198 0.49734858]\n",
      " [0.51449121 0.51096329 0.49735363]]\n",
      "[[0.51456751 0.51113261 0.49746032]\n",
      " [0.5145871  0.5111387  0.49744731]\n",
      " [0.51456255 0.51106628 0.49739358]\n",
      " [0.51451644 0.51098193 0.49734854]\n",
      " [0.51449123 0.51096335 0.49735368]]\n",
      "[[0.51456749 0.51113253 0.49746026]\n",
      " [0.5145871  0.51113871 0.49744731]\n",
      " [0.51456257 0.51106636 0.49739364]\n",
      " [0.51451646 0.51098202 0.49734861]\n",
      " [0.51449123 0.51096336 0.49735369]]\n",
      "[[0.51456753 0.51113265 0.49746035]\n",
      " [0.51458712 0.51113877 0.49744736]\n",
      " [0.51456256 0.51106631 0.4973936 ]\n",
      " [0.51451643 0.5109819  0.49734852]\n",
      " [0.51449121 0.51096328 0.49735363]]\n",
      "[[0.51454253 0.51113259 0.4974603 ]\n",
      " [0.51456213 0.51113874 0.49744734]\n",
      " [0.51453759 0.51106634 0.49739362]\n",
      " [0.51449147 0.51098196 0.49734856]\n",
      " [0.51446624 0.51096332 0.49735366]]\n",
      "[[0.51459249 0.51113259 0.4974603 ]\n",
      " [0.51461209 0.51113874 0.49744734]\n",
      " [0.51458755 0.51106634 0.49739362]\n",
      " [0.51454142 0.51098196 0.49734856]\n",
      " [0.5145162  0.51096332 0.49735366]]\n",
      "[[0.51455447 0.51113259 0.4974603 ]\n",
      " [0.51457414 0.51113874 0.49744734]\n",
      " [0.51454962 0.51106634 0.49739362]\n",
      " [0.51450346 0.51098196 0.49734856]\n",
      " [0.51447817 0.51096332 0.49735366]]\n",
      "[[0.51458054 0.51113259 0.4974603 ]\n",
      " [0.51460008 0.51113874 0.49744734]\n",
      " [0.51457551 0.51106634 0.49739362]\n",
      " [0.51452943 0.51098196 0.49734856]\n",
      " [0.51450427 0.51096332 0.49735366]]\n",
      "[[0.51455435 0.51113259 0.4974603 ]\n",
      " [0.51457401 0.51113874 0.49744734]\n",
      " [0.51454956 0.51106634 0.49739362]\n",
      " [0.51450349 0.51098196 0.49734856]\n",
      " [0.51447822 0.51096332 0.49735366]]\n",
      "[[0.51458067 0.51113259 0.4974603 ]\n",
      " [0.51460021 0.51113874 0.49744734]\n",
      " [0.51457557 0.51106634 0.49739362]\n",
      " [0.5145294  0.51098196 0.49734856]\n",
      " [0.51450422 0.51096332 0.49735366]]\n",
      "[[0.51455484 0.51113259 0.4974603 ]\n",
      " [0.51457443 0.51113874 0.49744734]\n",
      " [0.51454997 0.51106634 0.49739362]\n",
      " [0.51450395 0.51098196 0.49734856]\n",
      " [0.51447875 0.51096332 0.49735366]]\n",
      "[[0.51458018 0.51113259 0.4974603 ]\n",
      " [0.51459979 0.51113874 0.49744734]\n",
      " [0.51457516 0.51106634 0.49739362]\n",
      " [0.51452894 0.51098196 0.49734856]\n",
      " [0.51450369 0.51096332 0.49735366]]\n",
      "[[0.5145555  0.51113259 0.4974603 ]\n",
      " [0.51457503 0.51113874 0.49744734]\n",
      " [0.51455048 0.51106634 0.49739362]\n",
      " [0.51450441 0.51098196 0.49734856]\n",
      " [0.51447926 0.51096332 0.49735366]]\n",
      "[[0.51457952 0.51113259 0.4974603 ]\n",
      " [0.51459919 0.51113874 0.49744734]\n",
      " [0.51457465 0.51106634 0.49739362]\n",
      " [0.51452848 0.51098196 0.49734856]\n",
      " [0.51450319 0.51096332 0.49735366]]\n",
      "[[0.51455571 0.51113259 0.4974603 ]\n",
      " [0.51457525 0.51113874 0.49744734]\n",
      " [0.51455062 0.51106634 0.49739362]\n",
      " [0.51450445 0.51098196 0.49734856]\n",
      " [0.51447928 0.51096332 0.49735366]]\n",
      "[[0.5145793  0.51113259 0.4974603 ]\n",
      " [0.51459897 0.51113874 0.49744734]\n",
      " [0.51457452 0.51106634 0.49739362]\n",
      " [0.51452843 0.51098196 0.49734856]\n",
      " [0.51450316 0.51096332 0.49735366]]\n",
      "[[0.51456751 0.5111076  0.4974603 ]\n",
      " [0.51458711 0.51111375 0.49744734]\n",
      " [0.51456257 0.51104135 0.49739362]\n",
      " [0.51451644 0.51095697 0.49734856]\n",
      " [0.51449122 0.51093833 0.49735366]]\n",
      "[[0.51456751 0.51115758 0.4974603 ]\n",
      " [0.51458711 0.51116373 0.49744734]\n",
      " [0.51456257 0.51109132 0.49739362]\n",
      " [0.51451644 0.51100695 0.49734856]\n",
      " [0.51449122 0.51098831 0.49735366]]\n",
      "[[0.51456751 0.51111955 0.4974603 ]\n",
      " [0.51458711 0.51112577 0.49744734]\n",
      " [0.51456257 0.51105339 0.49739362]\n",
      " [0.51451644 0.51096897 0.49734856]\n",
      " [0.51449122 0.51095026 0.49735366]]\n",
      "[[0.51456751 0.51114563 0.4974603 ]\n",
      " [0.51458711 0.51115171 0.49744734]\n",
      " [0.51456257 0.51107928 0.49739362]\n",
      " [0.51451644 0.51099495 0.49734856]\n",
      " [0.51449122 0.51097638 0.49735366]]\n",
      "[[0.51456751 0.51111942 0.4974603 ]\n",
      " [0.51458711 0.51112563 0.49744734]\n",
      " [0.51456257 0.51105333 0.49739362]\n",
      " [0.51451644 0.510969   0.49734856]\n",
      " [0.51449122 0.51095032 0.49735366]]\n",
      "[[0.51456751 0.51114575 0.4974603 ]\n",
      " [0.51458711 0.51115185 0.49744734]\n",
      " [0.51456257 0.51107934 0.49739362]\n",
      " [0.51451644 0.51099492 0.49734856]\n",
      " [0.51449122 0.51097632 0.49735366]]\n",
      "[[0.51456751 0.51111992 0.4974603 ]\n",
      " [0.51458711 0.51112606 0.49744734]\n",
      " [0.51456257 0.51105374 0.49739362]\n",
      " [0.51451644 0.51096946 0.49734856]\n",
      " [0.51449122 0.51095084 0.49735366]]\n",
      "[[0.51456751 0.51114526 0.4974603 ]\n",
      " [0.51458711 0.51115142 0.49744734]\n",
      " [0.51456257 0.51107893 0.49739362]\n",
      " [0.51451644 0.51099446 0.49734856]\n",
      " [0.51449122 0.5109758  0.49735366]]\n",
      "[[0.51456751 0.51112057 0.4974603 ]\n",
      " [0.51458711 0.51112666 0.49744734]\n",
      " [0.51456257 0.51105425 0.49739362]\n",
      " [0.51451644 0.51096992 0.49734856]\n",
      " [0.51449122 0.51095135 0.49735366]]\n",
      "[[0.51456751 0.51114461 0.4974603 ]\n",
      " [0.51458711 0.51115082 0.49744734]\n",
      " [0.51456257 0.51107843 0.49739362]\n",
      " [0.51451644 0.51099399 0.49734856]\n",
      " [0.51449122 0.51097529 0.49735366]]\n",
      "[[0.51456751 0.51112079 0.4974603 ]\n",
      " [0.51458711 0.51112688 0.49744734]\n",
      " [0.51456257 0.51105438 0.49739362]\n",
      " [0.51451644 0.51096996 0.49734856]\n",
      " [0.51449122 0.51095138 0.49735366]]\n",
      "[[0.51456751 0.51114439 0.4974603 ]\n",
      " [0.51458711 0.5111506  0.49744734]\n",
      " [0.51456257 0.51107829 0.49739362]\n",
      " [0.51451644 0.51099395 0.49734856]\n",
      " [0.51449122 0.51097526 0.49735366]]\n",
      "[[0.51456751 0.51113259 0.49743531]\n",
      " [0.51458711 0.51113874 0.49742234]\n",
      " [0.51456257 0.51106634 0.49736862]\n",
      " [0.51451644 0.51098196 0.49732356]\n",
      " [0.51449122 0.51096332 0.49732866]]\n",
      "[[0.51456751 0.51113259 0.4974853 ]\n",
      " [0.51458711 0.51113874 0.49747234]\n",
      " [0.51456257 0.51106634 0.49741862]\n",
      " [0.51451644 0.51098196 0.49737356]\n",
      " [0.51449122 0.51096332 0.49737866]]\n",
      "[[0.51456751 0.51113259 0.49744726]\n",
      " [0.51458711 0.51113874 0.49743436]\n",
      " [0.51456257 0.51106634 0.49738067]\n",
      " [0.51451644 0.51098196 0.49733557]\n",
      " [0.51449122 0.51096332 0.49734059]]\n",
      "[[0.51456751 0.51113259 0.49747335]\n",
      " [0.51458711 0.51113874 0.49746032]\n",
      " [0.51456257 0.51106634 0.49740658]\n",
      " [0.51451644 0.51098196 0.49736156]\n",
      " [0.51449122 0.51096332 0.49736672]]\n",
      "[[0.51456751 0.51113259 0.49744713]\n",
      " [0.51458711 0.51113874 0.49743422]\n",
      " [0.51456257 0.51106634 0.49738061]\n",
      " [0.51451644 0.51098196 0.4973356 ]\n",
      " [0.51449122 0.51096332 0.49734065]]\n",
      "[[0.51456751 0.51113259 0.49747348]\n",
      " [0.51458711 0.51113874 0.49746045]\n",
      " [0.51456257 0.51106634 0.49740664]\n",
      " [0.51451644 0.51098196 0.49736153]\n",
      " [0.51449122 0.51096332 0.49736666]]\n",
      "[[0.51456751 0.51113259 0.49744762]\n",
      " [0.51458711 0.51113874 0.49743465]\n",
      " [0.51456257 0.51106634 0.49738102]\n",
      " [0.51451644 0.51098196 0.49733606]\n",
      " [0.51449122 0.51096332 0.49734117]]\n",
      "[[0.51456751 0.51113259 0.49747298]\n",
      " [0.51458711 0.51113874 0.49746002]\n",
      " [0.51456257 0.51106634 0.49740623]\n",
      " [0.51451644 0.51098196 0.49736107]\n",
      " [0.51449122 0.51096332 0.49736614]]\n",
      "[[0.51456751 0.51113259 0.49744828]\n",
      " [0.51458711 0.51113874 0.49743525]\n",
      " [0.51456257 0.51106634 0.49738153]\n",
      " [0.51451644 0.51098196 0.49733652]\n",
      " [0.51449122 0.51096332 0.49734168]]\n",
      "[[0.51456751 0.51113259 0.49747233]\n",
      " [0.51458711 0.51113874 0.49745942]\n",
      " [0.51456257 0.51106634 0.49740572]\n",
      " [0.51451644 0.51098196 0.49736061]\n",
      " [0.51449122 0.51096332 0.49736563]]\n",
      "[[0.51456751 0.51113259 0.4974485 ]\n",
      " [0.51458711 0.51113874 0.49743547]\n",
      " [0.51456257 0.51106634 0.49738166]\n",
      " [0.51451644 0.51098196 0.49733656]\n",
      " [0.51449122 0.51096332 0.49734171]]\n",
      "[[0.51456751 0.51113259 0.49747211]\n",
      " [0.51458711 0.51113874 0.4974592 ]\n",
      " [0.51456257 0.51106634 0.49740558]\n",
      " [0.51451644 0.51098196 0.49736056]\n",
      " [0.51449122 0.51096332 0.4973656 ]]\n",
      "[[-9.27825235e-03  0.00000000e+00]\n",
      " [-3.04978709e-06  0.00000000e+00]\n",
      " [-1.75060082e-04  0.00000000e+00]\n",
      " [-9.62660618e-05  0.00000000e+00]\n",
      " [ 8.89911959e-03  0.00000000e+00]\n",
      " [ 1.42869427e-05  0.00000000e+00]\n",
      " [ 2.33146358e-04  0.00000000e+00]\n",
      " [ 1.17982666e-04  0.00000000e+00]\n",
      " [-8.36010761e-03  0.00000000e+00]\n",
      " [-2.59383071e-05  0.00000000e+00]\n",
      " [-2.87468729e-04  0.00000000e+00]\n",
      " [-1.37149709e-04  0.00000000e+00]\n",
      " [ 7.62813551e-03  0.00000000e+00]\n",
      " [ 3.69883235e-05  0.00000000e+00]\n",
      " [ 3.35320347e-04  0.00000000e+00]\n",
      " [ 1.53247077e-04  0.00000000e+00]\n",
      " [-6.74798370e-03  0.00000000e+00]\n",
      " [-4.68759764e-05  0.00000000e+00]\n",
      " [-3.76215588e-04  0.00000000e+00]\n",
      " [-1.66560294e-04  0.00000000e+00]\n",
      " [ 3.14544970e-01  0.00000000e+00]\n",
      " [ 1.64090819e-01  0.00000000e+00]\n",
      " [ 1.64567932e-01  0.00000000e+00]\n",
      " [ 1.58339334e-01  0.00000000e+00]\n",
      " [ 1.51127527e-01  0.00000000e+00]\n",
      " [ 1.49568335e-01  0.00000000e+00]\n",
      " [ 1.11056588e-01  0.00000000e+00]\n",
      " [ 5.75736493e-02  0.00000000e+00]\n",
      " [ 5.77867378e-02  0.00000000e+00]\n",
      " [ 5.59235296e-02  0.00000000e+00]\n",
      " [ 5.36967009e-02  0.00000000e+00]\n",
      " [ 5.31542052e-02  0.00000000e+00]\n",
      " [ 9.74006970e-02  0.00000000e+00]\n",
      " [ 5.04575855e-02  0.00000000e+00]\n",
      " [ 5.07530173e-02  0.00000000e+00]\n",
      " [ 4.91620841e-02  0.00000000e+00]\n",
      " [ 4.71456249e-02  0.00000000e+00]\n",
      " [ 4.65597186e-02  0.00000000e+00]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 1\n"
     ]
    }
   ],
   "source": [
    "utils2.checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 2.5 Regularized Neural Network\n",
    "\n",
    "After you have successfully implemented the backpropagation algorithm, you will add regularization to the gradient. To account for regularization, it turns out that you can add this as an additional term *after* computing the gradients using backpropagation.\n",
    "\n",
    "Specifically, after you have computed $\\Delta_{ij}^{(l)}$ using backpropagation, you should add regularization using\n",
    "\n",
    "$$ \\begin{align} \n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that you should *not* be regularizing the first column of $\\Theta^{(l)}$ which is used for the bias term. Furthermore, in the parameters $\\Theta_{ij}^{(l)}$, $i$ is indexed starting from 1, and $j$ is indexed starting from 0. Thus, \n",
    "\n",
    "$$\n",
    "\\Theta^{(l)} = \\begin{bmatrix}\n",
    "\\Theta_{1,0}^{(i)} & \\Theta_{1,1}^{(l)} & \\cdots \\\\\n",
    "\\Theta_{2,0}^{(i)} & \\Theta_{2,1}^{(l)} & \\cdots \\\\\n",
    "\\vdots &  ~ & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "[Now modify your code that computes grad in `nnCostFunction` to account for regularization.](#nnCostFunction)\n",
    "\n",
    "After you are done, the following cell runs gradient checking on your implementation. If your code is correct, you should expect to see a relative difference that is less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51456751 0.51113259 0.4974603 ]\n",
      " [0.51458711 0.51113874 0.49744734]\n",
      " [0.51456257 0.51106634 0.49739362]\n",
      " [0.51451644 0.51098196 0.49734856]\n",
      " [0.51449122 0.51096332 0.49735366]]\n",
      "[[0.51456798 0.51113319 0.49746048]\n",
      " [0.51458758 0.51113934 0.49744751]\n",
      " [0.51456304 0.51106693 0.4973938 ]\n",
      " [0.51451692 0.51098256 0.49734874]\n",
      " [0.51449169 0.51096392 0.49735383]]\n",
      "[[0.51456704 0.51113199 0.49746013]\n",
      " [0.51458664 0.51113814 0.49744716]\n",
      " [0.51456209 0.51106574 0.49739345]\n",
      " [0.51451597 0.51098136 0.49734839]\n",
      " [0.51449075 0.51096272 0.49735348]]\n",
      "[[0.51456755 0.51113264 0.49746032]\n",
      " [0.51458715 0.51113879 0.49744735]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451641 0.51098191 0.49734855]\n",
      " [0.51449118 0.51096326 0.49735364]]\n",
      "[[0.51456747 0.51113254 0.49746029]\n",
      " [0.51458707 0.51113869 0.49744732]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451648 0.510982   0.49734858]\n",
      " [0.51449127 0.51096338 0.49735367]]\n",
      "[[0.5145675  0.51113257 0.4974603 ]\n",
      " [0.51458714 0.51113878 0.49744735]\n",
      " [0.51456261 0.51106639 0.49739364]\n",
      " [0.51451646 0.51098198 0.49734857]\n",
      " [0.5144912  0.51096329 0.49735365]]\n",
      "[[0.51456752 0.51113261 0.49746031]\n",
      " [0.51458708 0.5111387  0.49744733]\n",
      " [0.51456252 0.51106628 0.49739361]\n",
      " [0.51451643 0.51098193 0.49734856]\n",
      " [0.51449125 0.51096335 0.49735367]]\n",
      "[[0.51456746 0.51113253 0.49746029]\n",
      " [0.51458708 0.51113871 0.49744733]\n",
      " [0.51456259 0.51106636 0.49739363]\n",
      " [0.51451649 0.51098202 0.49734858]\n",
      " [0.51449125 0.51096336 0.49735367]]\n",
      "[[0.51456756 0.51113265 0.49746032]\n",
      " [0.51458713 0.51113877 0.49744735]\n",
      " [0.51456255 0.51106631 0.49739362]\n",
      " [0.5145164  0.5109819  0.49734855]\n",
      " [0.51449119 0.51096328 0.49735364]]\n",
      "[[0.5145671  0.51113197 0.49746005]\n",
      " [0.5145867  0.51113812 0.49744708]\n",
      " [0.51456216 0.51106572 0.49739337]\n",
      " [0.51451603 0.51098134 0.49734831]\n",
      " [0.51449081 0.5109627  0.4973534 ]]\n",
      "[[0.51456792 0.51113321 0.49746056]\n",
      " [0.51458752 0.51113936 0.49744759]\n",
      " [0.51456298 0.51106695 0.49739388]\n",
      " [0.51451685 0.51098258 0.49734882]\n",
      " [0.51449163 0.51096394 0.49735391]]\n",
      "[[0.51456747 0.51113254 0.49746028]\n",
      " [0.51458707 0.51113868 0.49744731]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451648 0.510982   0.49734858]\n",
      " [0.51449126 0.51096338 0.49735368]]\n",
      "[[0.51456754 0.51113264 0.49746033]\n",
      " [0.51458715 0.5111388  0.49744736]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451641 0.51098191 0.49734854]\n",
      " [0.51449118 0.51096326 0.49735363]]\n",
      "[[0.51456752 0.51113261 0.49746031]\n",
      " [0.51458708 0.5111387  0.49744732]\n",
      " [0.51456253 0.51106627 0.4973936 ]\n",
      " [0.51451643 0.51098193 0.49734855]\n",
      " [0.51449124 0.51096335 0.49735367]]\n",
      "[[0.5145675  0.51113257 0.4974603 ]\n",
      " [0.51458714 0.51113878 0.49744735]\n",
      " [0.51456261 0.5110664  0.49739365]\n",
      " [0.51451646 0.51098198 0.49734857]\n",
      " [0.5144912  0.51096329 0.49735364]]\n",
      "[[0.51456755 0.51113265 0.49746033]\n",
      " [0.51458713 0.51113877 0.49744735]\n",
      " [0.51456255 0.51106631 0.49739361]\n",
      " [0.5145164  0.5109819  0.49734854]\n",
      " [0.51449119 0.51096328 0.49735364]]\n",
      "[[0.51456747 0.51113253 0.49746028]\n",
      " [0.51458709 0.51113871 0.49744732]\n",
      " [0.51456258 0.51106636 0.49739363]\n",
      " [0.51451649 0.51098202 0.49734859]\n",
      " [0.51449125 0.51096336 0.49735367]]\n",
      "[[0.51456785 0.51113321 0.49746064]\n",
      " [0.51458745 0.51113936 0.49744767]\n",
      " [0.51456291 0.51106696 0.49739396]\n",
      " [0.51451678 0.51098258 0.4973489 ]\n",
      " [0.51449156 0.51096394 0.49735399]]\n",
      "[[0.51456717 0.51113196 0.49745997]\n",
      " [0.51458677 0.51113812 0.497447  ]\n",
      " [0.51456223 0.51106571 0.49739329]\n",
      " [0.5145161  0.51098133 0.49734823]\n",
      " [0.51449088 0.51096269 0.49735332]]\n",
      "[[0.51456754 0.51113264 0.49746033]\n",
      " [0.51458714 0.5111388  0.49744737]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451642 0.51098191 0.49734854]\n",
      " [0.51449119 0.51096326 0.49735362]]\n",
      "[[0.51456748 0.51113254 0.49746028]\n",
      " [0.51458708 0.51113868 0.49744731]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451647 0.51098201 0.49734859]\n",
      " [0.51449125 0.51096338 0.49735369]]\n",
      "[[0.5145675  0.51113257 0.49746029]\n",
      " [0.51458713 0.51113878 0.49744736]\n",
      " [0.5145626  0.5110664  0.49739366]\n",
      " [0.51451646 0.51098198 0.49734858]\n",
      " [0.5144912  0.51096328 0.49735364]]\n",
      "[[0.51456752 0.51113261 0.49746031]\n",
      " [0.51458709 0.5111387  0.49744732]\n",
      " [0.51456253 0.51106627 0.49739359]\n",
      " [0.51451643 0.51098193 0.49734855]\n",
      " [0.51449124 0.51096335 0.49735367]]\n",
      "[[0.51456748 0.51113253 0.49746027]\n",
      " [0.51458709 0.51113871 0.49744732]\n",
      " [0.51456258 0.51106636 0.49739364]\n",
      " [0.51451648 0.51098202 0.4973486 ]\n",
      " [0.51449124 0.51096336 0.49735368]]\n",
      "[[0.51456754 0.51113265 0.49746034]\n",
      " [0.51458713 0.51113877 0.49744736]\n",
      " [0.51456255 0.51106631 0.49739361]\n",
      " [0.51451641 0.5109819  0.49734853]\n",
      " [0.5144912  0.51096328 0.49735363]]\n",
      "[[0.51456725 0.51113197 0.4974599 ]\n",
      " [0.51458685 0.51113812 0.49744693]\n",
      " [0.5145623  0.51106572 0.49739322]\n",
      " [0.51451618 0.51098134 0.49734816]\n",
      " [0.51449096 0.5109627  0.49735325]]\n",
      "[[0.51456777 0.51113321 0.49746071]\n",
      " [0.51458737 0.51113936 0.49744774]\n",
      " [0.51456283 0.51106695 0.49739403]\n",
      " [0.51451671 0.51098258 0.49734897]\n",
      " [0.51449148 0.51096394 0.49735406]]\n",
      "[[0.51456749 0.51113254 0.49746027]\n",
      " [0.51458709 0.51113868 0.4974473 ]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451646 0.51098201 0.49734859]\n",
      " [0.51449125 0.51096338 0.4973537 ]]\n",
      "[[0.51456753 0.51113264 0.49746034]\n",
      " [0.51458713 0.5111388  0.49744737]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451642 0.51098191 0.49734853]\n",
      " [0.5144912  0.51096326 0.49735362]]\n",
      "[[0.51456752 0.51113261 0.49746032]\n",
      " [0.51458709 0.5111387  0.49744731]\n",
      " [0.51456254 0.51106627 0.49739358]\n",
      " [0.51451643 0.51098193 0.49734855]\n",
      " [0.51449124 0.51096335 0.49735368]]\n",
      "[[0.5145675  0.51113257 0.49746029]\n",
      " [0.51458713 0.51113878 0.49744736]\n",
      " [0.51456259 0.5110664  0.49739366]\n",
      " [0.51451646 0.51098198 0.49734858]\n",
      " [0.51449121 0.51096329 0.49735363]]\n",
      "[[0.51456754 0.51113265 0.49746034]\n",
      " [0.51458712 0.51113877 0.49744736]\n",
      " [0.51456256 0.51106631 0.49739361]\n",
      " [0.51451642 0.5109819  0.49734852]\n",
      " [0.5144912  0.51096328 0.49735363]]\n",
      "[[0.51456748 0.51113253 0.49746026]\n",
      " [0.5145871  0.51113871 0.49744732]\n",
      " [0.51456258 0.51106636 0.49739364]\n",
      " [0.51451647 0.51098202 0.4973486 ]\n",
      " [0.51449124 0.51096336 0.49735368]]\n",
      "[[0.51456769 0.51113319 0.49746077]\n",
      " [0.51458729 0.51113934 0.49744781]\n",
      " [0.51456275 0.51106693 0.49739409]\n",
      " [0.51451662 0.51098256 0.49734903]\n",
      " [0.5144914  0.51096392 0.49735412]]\n",
      "[[0.51456733 0.51113199 0.49745984]\n",
      " [0.51458693 0.51113814 0.49744687]\n",
      " [0.51456239 0.51106574 0.49739316]\n",
      " [0.51451627 0.51098136 0.49734809]\n",
      " [0.51449104 0.51096272 0.49735319]]\n",
      "[[0.51456752 0.51113264 0.49746034]\n",
      " [0.51458713 0.51113879 0.49744738]\n",
      " [0.51456257 0.51106634 0.49739363]\n",
      " [0.51451643 0.51098191 0.49734853]\n",
      " [0.5144912  0.51096326 0.49735361]]\n",
      "[[0.51456749 0.51113254 0.49746026]\n",
      " [0.51458709 0.51113869 0.49744729]\n",
      " [0.51456256 0.51106633 0.49739362]\n",
      " [0.51451646 0.510982   0.4973486 ]\n",
      " [0.51449124 0.51096338 0.4973537 ]]\n",
      "[[0.5145675  0.51113257 0.49746029]\n",
      " [0.51458712 0.51113878 0.49744737]\n",
      " [0.51456258 0.51106639 0.49739367]\n",
      " [0.51451645 0.51098198 0.49734858]\n",
      " [0.51449121 0.51096329 0.49735363]]\n",
      "[[0.51456751 0.51113261 0.49746032]\n",
      " [0.5145871  0.5111387  0.49744731]\n",
      " [0.51456255 0.51106628 0.49739358]\n",
      " [0.51451644 0.51098193 0.49734854]\n",
      " [0.51449123 0.51096335 0.49735368]]\n",
      "[[0.51456749 0.51113253 0.49746026]\n",
      " [0.5145871  0.51113871 0.49744731]\n",
      " [0.51456257 0.51106636 0.49739364]\n",
      " [0.51451646 0.51098202 0.49734861]\n",
      " [0.51449123 0.51096336 0.49735369]]\n",
      "[[0.51456753 0.51113265 0.49746035]\n",
      " [0.51458712 0.51113877 0.49744736]\n",
      " [0.51456256 0.51106631 0.4973936 ]\n",
      " [0.51451643 0.5109819  0.49734852]\n",
      " [0.51449121 0.51096328 0.49735363]]\n",
      "[[0.51454253 0.51113259 0.4974603 ]\n",
      " [0.51456213 0.51113874 0.49744734]\n",
      " [0.51453759 0.51106634 0.49739362]\n",
      " [0.51449147 0.51098196 0.49734856]\n",
      " [0.51446624 0.51096332 0.49735366]]\n",
      "[[0.51459249 0.51113259 0.4974603 ]\n",
      " [0.51461209 0.51113874 0.49744734]\n",
      " [0.51458755 0.51106634 0.49739362]\n",
      " [0.51454142 0.51098196 0.49734856]\n",
      " [0.5145162  0.51096332 0.49735366]]\n",
      "[[0.51455447 0.51113259 0.4974603 ]\n",
      " [0.51457414 0.51113874 0.49744734]\n",
      " [0.51454962 0.51106634 0.49739362]\n",
      " [0.51450346 0.51098196 0.49734856]\n",
      " [0.51447817 0.51096332 0.49735366]]\n",
      "[[0.51458054 0.51113259 0.4974603 ]\n",
      " [0.51460008 0.51113874 0.49744734]\n",
      " [0.51457551 0.51106634 0.49739362]\n",
      " [0.51452943 0.51098196 0.49734856]\n",
      " [0.51450427 0.51096332 0.49735366]]\n",
      "[[0.51455435 0.51113259 0.4974603 ]\n",
      " [0.51457401 0.51113874 0.49744734]\n",
      " [0.51454956 0.51106634 0.49739362]\n",
      " [0.51450349 0.51098196 0.49734856]\n",
      " [0.51447822 0.51096332 0.49735366]]\n",
      "[[0.51458067 0.51113259 0.4974603 ]\n",
      " [0.51460021 0.51113874 0.49744734]\n",
      " [0.51457557 0.51106634 0.49739362]\n",
      " [0.5145294  0.51098196 0.49734856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0.51450422 0.51096332 0.49735366]]\n",
      "[[0.51455484 0.51113259 0.4974603 ]\n",
      " [0.51457443 0.51113874 0.49744734]\n",
      " [0.51454997 0.51106634 0.49739362]\n",
      " [0.51450395 0.51098196 0.49734856]\n",
      " [0.51447875 0.51096332 0.49735366]]\n",
      "[[0.51458018 0.51113259 0.4974603 ]\n",
      " [0.51459979 0.51113874 0.49744734]\n",
      " [0.51457516 0.51106634 0.49739362]\n",
      " [0.51452894 0.51098196 0.49734856]\n",
      " [0.51450369 0.51096332 0.49735366]]\n",
      "[[0.5145555  0.51113259 0.4974603 ]\n",
      " [0.51457503 0.51113874 0.49744734]\n",
      " [0.51455048 0.51106634 0.49739362]\n",
      " [0.51450441 0.51098196 0.49734856]\n",
      " [0.51447926 0.51096332 0.49735366]]\n",
      "[[0.51457952 0.51113259 0.4974603 ]\n",
      " [0.51459919 0.51113874 0.49744734]\n",
      " [0.51457465 0.51106634 0.49739362]\n",
      " [0.51452848 0.51098196 0.49734856]\n",
      " [0.51450319 0.51096332 0.49735366]]\n",
      "[[0.51455571 0.51113259 0.4974603 ]\n",
      " [0.51457525 0.51113874 0.49744734]\n",
      " [0.51455062 0.51106634 0.49739362]\n",
      " [0.51450445 0.51098196 0.49734856]\n",
      " [0.51447928 0.51096332 0.49735366]]\n",
      "[[0.5145793  0.51113259 0.4974603 ]\n",
      " [0.51459897 0.51113874 0.49744734]\n",
      " [0.51457452 0.51106634 0.49739362]\n",
      " [0.51452843 0.51098196 0.49734856]\n",
      " [0.51450316 0.51096332 0.49735366]]\n",
      "[[0.51456751 0.5111076  0.4974603 ]\n",
      " [0.51458711 0.51111375 0.49744734]\n",
      " [0.51456257 0.51104135 0.49739362]\n",
      " [0.51451644 0.51095697 0.49734856]\n",
      " [0.51449122 0.51093833 0.49735366]]\n",
      "[[0.51456751 0.51115758 0.4974603 ]\n",
      " [0.51458711 0.51116373 0.49744734]\n",
      " [0.51456257 0.51109132 0.49739362]\n",
      " [0.51451644 0.51100695 0.49734856]\n",
      " [0.51449122 0.51098831 0.49735366]]\n",
      "[[0.51456751 0.51111955 0.4974603 ]\n",
      " [0.51458711 0.51112577 0.49744734]\n",
      " [0.51456257 0.51105339 0.49739362]\n",
      " [0.51451644 0.51096897 0.49734856]\n",
      " [0.51449122 0.51095026 0.49735366]]\n",
      "[[0.51456751 0.51114563 0.4974603 ]\n",
      " [0.51458711 0.51115171 0.49744734]\n",
      " [0.51456257 0.51107928 0.49739362]\n",
      " [0.51451644 0.51099495 0.49734856]\n",
      " [0.51449122 0.51097638 0.49735366]]\n",
      "[[0.51456751 0.51111942 0.4974603 ]\n",
      " [0.51458711 0.51112563 0.49744734]\n",
      " [0.51456257 0.51105333 0.49739362]\n",
      " [0.51451644 0.510969   0.49734856]\n",
      " [0.51449122 0.51095032 0.49735366]]\n",
      "[[0.51456751 0.51114575 0.4974603 ]\n",
      " [0.51458711 0.51115185 0.49744734]\n",
      " [0.51456257 0.51107934 0.49739362]\n",
      " [0.51451644 0.51099492 0.49734856]\n",
      " [0.51449122 0.51097632 0.49735366]]\n",
      "[[0.51456751 0.51111992 0.4974603 ]\n",
      " [0.51458711 0.51112606 0.49744734]\n",
      " [0.51456257 0.51105374 0.49739362]\n",
      " [0.51451644 0.51096946 0.49734856]\n",
      " [0.51449122 0.51095084 0.49735366]]\n",
      "[[0.51456751 0.51114526 0.4974603 ]\n",
      " [0.51458711 0.51115142 0.49744734]\n",
      " [0.51456257 0.51107893 0.49739362]\n",
      " [0.51451644 0.51099446 0.49734856]\n",
      " [0.51449122 0.5109758  0.49735366]]\n",
      "[[0.51456751 0.51112057 0.4974603 ]\n",
      " [0.51458711 0.51112666 0.49744734]\n",
      " [0.51456257 0.51105425 0.49739362]\n",
      " [0.51451644 0.51096992 0.49734856]\n",
      " [0.51449122 0.51095135 0.49735366]]\n",
      "[[0.51456751 0.51114461 0.4974603 ]\n",
      " [0.51458711 0.51115082 0.49744734]\n",
      " [0.51456257 0.51107843 0.49739362]\n",
      " [0.51451644 0.51099399 0.49734856]\n",
      " [0.51449122 0.51097529 0.49735366]]\n",
      "[[0.51456751 0.51112079 0.4974603 ]\n",
      " [0.51458711 0.51112688 0.49744734]\n",
      " [0.51456257 0.51105438 0.49739362]\n",
      " [0.51451644 0.51096996 0.49734856]\n",
      " [0.51449122 0.51095138 0.49735366]]\n",
      "[[0.51456751 0.51114439 0.4974603 ]\n",
      " [0.51458711 0.5111506  0.49744734]\n",
      " [0.51456257 0.51107829 0.49739362]\n",
      " [0.51451644 0.51099395 0.49734856]\n",
      " [0.51449122 0.51097526 0.49735366]]\n",
      "[[0.51456751 0.51113259 0.49743531]\n",
      " [0.51458711 0.51113874 0.49742234]\n",
      " [0.51456257 0.51106634 0.49736862]\n",
      " [0.51451644 0.51098196 0.49732356]\n",
      " [0.51449122 0.51096332 0.49732866]]\n",
      "[[0.51456751 0.51113259 0.4974853 ]\n",
      " [0.51458711 0.51113874 0.49747234]\n",
      " [0.51456257 0.51106634 0.49741862]\n",
      " [0.51451644 0.51098196 0.49737356]\n",
      " [0.51449122 0.51096332 0.49737866]]\n",
      "[[0.51456751 0.51113259 0.49744726]\n",
      " [0.51458711 0.51113874 0.49743436]\n",
      " [0.51456257 0.51106634 0.49738067]\n",
      " [0.51451644 0.51098196 0.49733557]\n",
      " [0.51449122 0.51096332 0.49734059]]\n",
      "[[0.51456751 0.51113259 0.49747335]\n",
      " [0.51458711 0.51113874 0.49746032]\n",
      " [0.51456257 0.51106634 0.49740658]\n",
      " [0.51451644 0.51098196 0.49736156]\n",
      " [0.51449122 0.51096332 0.49736672]]\n",
      "[[0.51456751 0.51113259 0.49744713]\n",
      " [0.51458711 0.51113874 0.49743422]\n",
      " [0.51456257 0.51106634 0.49738061]\n",
      " [0.51451644 0.51098196 0.4973356 ]\n",
      " [0.51449122 0.51096332 0.49734065]]\n",
      "[[0.51456751 0.51113259 0.49747348]\n",
      " [0.51458711 0.51113874 0.49746045]\n",
      " [0.51456257 0.51106634 0.49740664]\n",
      " [0.51451644 0.51098196 0.49736153]\n",
      " [0.51449122 0.51096332 0.49736666]]\n",
      "[[0.51456751 0.51113259 0.49744762]\n",
      " [0.51458711 0.51113874 0.49743465]\n",
      " [0.51456257 0.51106634 0.49738102]\n",
      " [0.51451644 0.51098196 0.49733606]\n",
      " [0.51449122 0.51096332 0.49734117]]\n",
      "[[0.51456751 0.51113259 0.49747298]\n",
      " [0.51458711 0.51113874 0.49746002]\n",
      " [0.51456257 0.51106634 0.49740623]\n",
      " [0.51451644 0.51098196 0.49736107]\n",
      " [0.51449122 0.51096332 0.49736614]]\n",
      "[[0.51456751 0.51113259 0.49744828]\n",
      " [0.51458711 0.51113874 0.49743525]\n",
      " [0.51456257 0.51106634 0.49738153]\n",
      " [0.51451644 0.51098196 0.49733652]\n",
      " [0.51449122 0.51096332 0.49734168]]\n",
      "[[0.51456751 0.51113259 0.49747233]\n",
      " [0.51458711 0.51113874 0.49745942]\n",
      " [0.51456257 0.51106634 0.49740572]\n",
      " [0.51451644 0.51098196 0.49736061]\n",
      " [0.51449122 0.51096332 0.49736563]]\n",
      "[[0.51456751 0.51113259 0.4974485 ]\n",
      " [0.51458711 0.51113874 0.49743547]\n",
      " [0.51456257 0.51106634 0.49738166]\n",
      " [0.51451644 0.51098196 0.49733656]\n",
      " [0.51449122 0.51096332 0.49734171]]\n",
      "[[0.51456751 0.51113259 0.49747211]\n",
      " [0.51458711 0.51113874 0.4974592 ]\n",
      " [0.51456257 0.51106634 0.49740558]\n",
      " [0.51451644 0.51098196 0.49736056]\n",
      " [0.51449122 0.51096332 0.4973656 ]]\n",
      "[[-9.27825235e-03  0.00000000e+00]\n",
      " [-1.67679797e-02  0.00000000e+00]\n",
      " [-6.01744725e-02  0.00000000e+00]\n",
      " [-1.73704651e-02  0.00000000e+00]\n",
      " [ 8.89911959e-03  0.00000000e+00]\n",
      " [ 3.94334829e-02  0.00000000e+00]\n",
      " [-3.19612287e-02  0.00000000e+00]\n",
      " [-5.75658668e-02  0.00000000e+00]\n",
      " [-8.36010761e-03  0.00000000e+00]\n",
      " [ 5.93355565e-02  0.00000000e+00]\n",
      " [ 2.49225535e-02  0.00000000e+00]\n",
      " [-4.51963845e-02  0.00000000e+00]\n",
      " [ 7.62813551e-03  0.00000000e+00]\n",
      " [ 2.47640974e-02  0.00000000e+00]\n",
      " [ 5.97717617e-02  0.00000000e+00]\n",
      " [ 9.14587966e-03  0.00000000e+00]\n",
      " [-6.74798370e-03  0.00000000e+00]\n",
      " [-3.26881426e-02  0.00000000e+00]\n",
      " [ 3.86410548e-02  0.00000000e+00]\n",
      " [ 5.46101547e-02  0.00000000e+00]\n",
      " [ 3.14544970e-01  0.00000000e+00]\n",
      " [ 1.18682669e-01  0.00000000e+00]\n",
      " [ 2.03987128e-01  0.00000000e+00]\n",
      " [ 1.25698067e-01  0.00000000e+00]\n",
      " [ 1.76337550e-01  0.00000000e+00]\n",
      " [ 1.32294136e-01  0.00000000e+00]\n",
      " [ 1.11056588e-01  0.00000000e+00]\n",
      " [ 3.81928666e-05  0.00000000e+00]\n",
      " [ 1.17148233e-01  0.00000000e+00]\n",
      " [-4.07588279e-03  0.00000000e+00]\n",
      " [ 1.13133142e-01  0.00000000e+00]\n",
      " [-4.52964427e-03  0.00000000e+00]\n",
      " [ 9.74006970e-02  0.00000000e+00]\n",
      " [ 3.36926556e-02  0.00000000e+00]\n",
      " [ 7.54801264e-02  0.00000000e+00]\n",
      " [ 1.69677090e-02  0.00000000e+00]\n",
      " [ 8.61628953e-02  0.00000000e+00]\n",
      " [ 1.50048382e-03  0.00000000e+00]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 1\n",
      "[[9.95734012e-01 1.12661530e-04 1.74127856e-03 ... 5.51517524e-03\n",
      "  4.01468105e-04 6.48072305e-03]\n",
      " [9.95696931e-01 4.79026796e-04 2.41495958e-03 ... 1.15788527e-02\n",
      "  2.39107046e-03 1.97025086e-03]\n",
      " [9.28008397e-01 8.85702310e-05 3.24266731e-03 ... 3.86839058e-04\n",
      "  6.22892325e-02 5.49803551e-03]\n",
      " ...\n",
      " [2.42384687e-05 5.17641791e-02 3.81715020e-03 ... 1.44301919e-04\n",
      "  2.15667361e-03 6.49826950e-01]\n",
      " [2.06173648e-04 8.30631310e-04 6.22003774e-04 ... 1.20516046e-02\n",
      "  1.19366192e-02 9.71410499e-01]\n",
      " [8.18576980e-02 4.81465717e-05 4.58821829e-04 ... 3.69700393e-02\n",
      "  5.73434571e-03 6.96288990e-01]]\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 0.576051 \n",
      "(for lambda = 3, this value should be about 0.576051)\n"
     ]
    }
   ],
   "source": [
    "#  Check gradients by running checkNNGradients\n",
    "lambda_ = 3\n",
    "utils2.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n",
    "print('(for lambda = 3, this value should be about 0.576051)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go ahead\n"
     ]
    }
   ],
   "source": [
    "if(abs(debug_J - 0.576) > 0.3):\n",
    "    print(\"You should check your code\")\n",
    "else:\n",
    "    print(\"You are good to go ahead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Learning parameters using `scipy.optimize.minimize`\n",
    "\n",
    "After you have successfully implemented the neural network cost function\n",
    "and gradient computation, the next step we will use `scipy`'s minimization to learn a good set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " ...\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]]\n",
      "[[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " ...\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training completes, we will proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct. If your implementation is correct, you should see a reported\n",
    "training accuracy of about 95.3% (this may vary by about 1% due to the random initialization). It is possible to get higher training accuracies by training the neural network for more iterations. We encourage you to try\n",
    "training the neural network for more iterations (e.g., set `maxiter` to 400) and also vary the regularization parameter $\\lambda$. With the right learning settings, it is possible to get the neural network to perfectly fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 10.000000\n"
     ]
    }
   ],
   "source": [
    "pred = utils2.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualizing the Hidden Layer\n",
    "\n",
    "One way to understand what your neural network is learning is to visualize what the representations captured by the hidden units. Informally, given a particular hidden unit, one way to visualize what it computes is to find an input $x$ that will cause it to activate (that is, to have an activation value \n",
    "($a_i^{(l)}$) close to 1). For the neural network you trained, notice that the $i^{th}$ row of $\\Theta^{(1)}$ is a 401-dimensional vector that represents the parameter for the $i^{th}$ hidden unit. If we discard the bias term, we get a 400 dimensional vector that represents the weights from each input pixel to the hidden unit.\n",
    "\n",
    "Thus, one way to visualize the “representation” captured by the hidden unit is to reshape this 400 dimensional vector into a 20 × 20 image and display it (It turns out that this is equivalent to finding the input that gives the highest activation for the hidden unit, given a “norm” constraint on the input (i.e., $||x||_2 \\le 1$)). \n",
    "\n",
    "The next cell does this by using the `displayData` function and it will show you an image with 25 units,\n",
    "each corresponding to one hidden unit in the network. In your trained network, you should find that the hidden units corresponds roughly to detectors that look for strokes and other patterns in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils2.displayData(Theta1[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optional (ungraded) exercise\n",
    "\n",
    "In this part of the exercise, you will get to try out different learning settings for the neural network to see how the performance of the neural network varies with the regularization parameter $\\lambda$ and number of training steps (the `maxiter` option when using `scipy.optimize.minimize`). Neural networks are very powerful models that can form highly complex decision boundaries. Without regularization, it is possible for a neural network to “overfit” a training set so that it obtains close to 100% accuracy on the training set but does not as well on new examples that it has not seen before. You can set the regularization $\\lambda$ to a smaller value and the `maxiter` parameter to a higher number of iterations to see this for youself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
